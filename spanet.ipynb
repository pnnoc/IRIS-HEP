{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: dealing with jets symmetries (quark)\n",
    "\n",
    "Special Loss function: dealing with particle symmetries (top and anti-top)\n",
    "\n",
    "Tensor Attention: endoding the symmetries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: unordered jets\n",
    "Outpu: one head per particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import awkward as ak\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from coffea to h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx1 => SubJet\n",
      "  warnings.warn(\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx2 => SubJet\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#writing h5 from coffea schema\n",
    "# file_path = 'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root' #for training\n",
    "file_path = 'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0001.root' #for testing\n",
    "tree_name = 'Events'\n",
    "# events = NanoEventsFactory.from_root({file_path: tree_name}, schemaclass=NanoAODSchema).events()\n",
    "events = NanoEventsFactory.from_root(file_path, treepath=tree_name, schemaclass=NanoAODSchema).events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from jetassignment_training\n",
    "# events filtering\n",
    "selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta)<2.1) & \n",
    "                                        (events.Electron.cutBased==4) & (events.Electron.sip3d < 4)]\n",
    "selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta)<2.1) & (events.Muon.tightId) & \n",
    "                                (events.Muon.sip3d < 4) & (events.Muon.pfRelIso04_all < 0.15)]\n",
    "jet_filter = (events.Jet.pt > 30) & (np.abs(events.Jet.eta) < 2.4) & (events.Jet.isTightLeptonVeto)\n",
    "selected_jets = events.Jet[jet_filter]\n",
    "selected_genpart = events.GenPart\n",
    "# even = (events.event%2==0)\n",
    "    \n",
    "# single lepton requirement\n",
    "event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "# require at least 4 jets\n",
    "event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "# require at least one jet above B_TAG_THRESHOLD\n",
    "B_TAG_THRESHOLD = 0.5\n",
    "event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "    \n",
    "# apply event filters\n",
    "selected_electrons = selected_electrons[event_filters]\n",
    "selected_muons = selected_muons[event_filters]\n",
    "selected_jets = selected_jets[event_filters]\n",
    "selected_genpart = selected_genpart[event_filters]\n",
    "# even = even[event_filters]\n",
    "    \n",
    "### only consider 4j2b (signal) region\n",
    "region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2 # at least two b-tagged jets\n",
    "selected_jets_region = selected_jets[region_filter] # all jet\n",
    "selected_electrons_region = selected_electrons[region_filter]\n",
    "selected_muons_region = selected_muons[region_filter]\n",
    "selected_genpart_region = selected_genpart[region_filter]\n",
    "# even = even[region_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_filter_multiple_jets(jets, electrons, muons, genparts):\n",
    "    '''\n",
    "    Filters events down to training set and calculates jet-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after region filter (and selecting leading four for each event)\n",
    "        electrons: selected electrons after region filter\n",
    "        muons: selected muons after region filter\n",
    "        genparts: selected genpart after region filter\n",
    "        even: whether the event is even-numbered (used to separate training events)\n",
    "    \n",
    "    Returns:\n",
    "        jets: selected jets after training filter\n",
    "        electrons: selected electrons after training filter\n",
    "        muons: selected muons after training filter\n",
    "        labels: labels of jets within an event (24=W, 6=top_hadron, -6=top_lepton)\n",
    "        even: whether the event is even-numbered\n",
    "    '''\n",
    "    #### filter genPart to valid matching candidates ####\n",
    "\n",
    "    # get rid of particles without parents\n",
    "    genpart_parent = genparts.distinctParent\n",
    "    genpart_filter = np.invert(ak.is_none(genpart_parent, axis=1))\n",
    "    genparts = genparts[genpart_filter]\n",
    "    genpart_parent = genparts.distinctParent\n",
    "\n",
    "    # ensure that parents are top quark or W\n",
    "    genpart_filter2 = ((np.abs(genpart_parent.pdgId)==6) | (np.abs(genpart_parent.pdgId)==24))\n",
    "    genparts = genparts[genpart_filter2]\n",
    "\n",
    "    # ensure particle itself is a quark\n",
    "    genpart_filter3 = ((np.abs(genparts.pdgId)<7) & (np.abs(genparts.pdgId)>0))\n",
    "    genparts = genparts[genpart_filter3]\n",
    "\n",
    "    # get rid of duplicates\n",
    "    genpart_filter4 = genparts.hasFlags(\"isLastCopy\")\n",
    "    genparts = genparts[genpart_filter4]\n",
    "            \n",
    "        \n",
    "    #### get jet-level labels and filter events to training set\n",
    "        \n",
    "    # match jets to nearest valid genPart candidate\n",
    "    nearest_genpart = jets.nearest(genparts, threshold=0.4)\n",
    "    nearest_parent = nearest_genpart.distinctParent # parent of matched particle\n",
    "    parent_pdgid = nearest_parent.pdgId # pdgId of parent particle\n",
    "    grandchild_pdgid = nearest_parent.distinctChildren.distinctChildren.pdgId # pdgId of particle's parent's grandchildren\n",
    "\n",
    "    grandchildren_flat = np.abs(ak.flatten(grandchild_pdgid,axis=-1)) # flatten innermost axis for convenience\n",
    "\n",
    "    # if particle has a cousin that is a lepton\n",
    "    has_lepton_cousin = (ak.sum(((grandchildren_flat%2==0) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                axis=-1)>0)\n",
    "    # if particle has a cousin that is a neutrino\n",
    "    has_neutrino_cousin = (ak.sum(((grandchildren_flat%2==1) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                  axis=-1)>0)\n",
    "\n",
    "    # if a particle has a lepton cousin and a neutrino cousin\n",
    "    has_both_cousins = ak.fill_none((has_lepton_cousin & has_neutrino_cousin), False) #not using .to_numpy bc inregular array size (different event level, multiple jets)\n",
    "\n",
    "    # get labels from parent pdgId (fill none with 100 to filter out events with those jets)\n",
    "    labels = np.abs(ak.fill_none(parent_pdgid,100)) #not using .to_numpy bc inregular array size (different event level, multiple jets)\n",
    "\n",
    "    # changing the labels while still preserve awkward array jets. To bypass inplace assignment error of numpy_array vs awkward_array\n",
    "    new_labels = ak.Array([])\n",
    "    for idx in range(len(labels)):\n",
    "        (labels[idx].to_numpy())[has_both_cousins[idx].to_numpy()]=-6\n",
    "        new_labels = ak.concatenate([new_labels, ak.Array([labels[idx]])], axis=0)\n",
    "    labels = new_labels\n",
    "    \n",
    "    #mask for event validation (atleast 4 jets with -6, 6, 24, 24, 100, 100,...)\n",
    "    mask = ak.Array([])\n",
    "    for idx in range(len(labels)):\n",
    "        event_valid_bool = (ak.sum(labels[idx]==-6)==1) & (ak.sum(labels[idx]==6)==1) & (ak.sum(labels[idx]==24)==2)\n",
    "        mask = ak.concatenate([mask, ak.Array([event_valid_bool])], axis=0)\n",
    "            \n",
    "    # filter events\n",
    "    jets = jets[mask]\n",
    "    electrons = electrons[mask]\n",
    "    muons = muons[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    return jets, electrons, muons, labels\n",
    "\n",
    "def pad_array(array, max_jets=12, pad_value=0): #padded arra yfor features, except MASK\n",
    "    padded_array = ak.pad_none(array, max_jets, axis=1)\n",
    "    padded_array_filled = ak.fill_none(padded_array, pad_value, axis=1)\n",
    "    return padded_array_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting labels for valid event\n",
    "jets, electrons, muons, labels = training_filter_multiple_jets(selected_jets_region, selected_electrons_region, selected_muons_region, selected_genpart_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating padded features for each event\n",
    "Jets_pt_padded = pad_array(jets.pt)\n",
    "Jets_mass_padded = pad_array(jets.mass)\n",
    "Jets_eta_padded = pad_array(jets.eta)\n",
    "Jets_phi_padded = pad_array(jets.phi)\n",
    "Jets_btag_padded = pad_array(jets.btagCSVV2)\n",
    "Jets_qgl_padded = pad_array(jets.qgl)\n",
    "\n",
    "#lepton features (only electrons left)\n",
    "#Since there's only one lepton, might not need to pad\n",
    "lep_pt_padded = pad_array(electrons.pt)\n",
    "lep_mass_padded = pad_array(electrons.mass)\n",
    "lep_eta_padded = pad_array(electrons.eta)\n",
    "lep_phi_padded = pad_array(electrons.phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating MASK for each Jet event\n",
    "pad_check = ak.pad_none(jets, 12, axis=1)\n",
    "Jets_mask_padded = np.invert(ak.is_none(pad_check, axis=1))\n",
    "\n",
    "#creating MASK for each Lepton event\n",
    "lep_mask_padded = np.invert(ak.is_none(pad_check, axis=1)) #not sure if this is necessary\n",
    "\n",
    "#labels padded (missing jets should be label as -1 ?) #probably dont need padded_labels to preserve memory (only need to extract indices)\n",
    "labels_padded = pad_array(labels, pad_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target indices fot jets and lepton\n",
    "def label_indices(labels):\n",
    "    l_indices = ak.Array([])\n",
    "    b_lep_indices = ak.Array([])\n",
    "    b_top_indices = ak.Array([])\n",
    "    w1_indices = ak.Array([])\n",
    "    w2_indices = ak.Array([])\n",
    "    \n",
    "    \n",
    "    for event in labels:\n",
    "        l_index = ak.Array([0]) #always 0th because only 1 lepton(electron) in each event. lepton target associated with lepton sequential input\n",
    "        b_lep_index = ak.where(event==-6) #b_top_lep\n",
    "        b_top_index = ak.where(event==6) #b_top_had\n",
    "        w_index = ak.where(event==24)#giving two indices of W jets\n",
    "        \n",
    "        l_indices = ak.concatenate([l_indices, l_index], axis=0) # worked (alreay flatten)\n",
    "        b_lep_indices = ak.concatenate([b_lep_indices, b_lep_index], axis=0)\n",
    "        b_top_indices = ak.concatenate([b_top_indices, b_top_index], axis=0)\n",
    "        \n",
    "        w1_indices = ak.concatenate([w1_indices, ak.Array([w_index[0][0]])], axis=0) #worked (alreay flatten)\n",
    "        w2_indices = ak.concatenate([w2_indices, ak.Array([w_index[0][1]])], axis=0) #worked (alreay flatten)\n",
    "    \n",
    "    b_lep_indices = ak.flatten(b_lep_indices)\n",
    "    b_top_indices = ak.flatten(b_top_indices)\n",
    "        \n",
    "    return l_indices, b_lep_indices, b_top_indices, w1_indices, w2_indices\n",
    "       \n",
    "#     labels_indices = ak.Array([])\n",
    "    \n",
    "#     for event in labels:\n",
    "#         l_index = ak.Array([[0]]) #always 0th because only 1 lepton(electron) in each event. lepton target associated with lepton sequential input\n",
    "#         b_lep_index = ak.where(event==-6) #b_top_lep\n",
    "#         b_top_index = ak.where(event==6) #b_top_had\n",
    "#         w_index = ak.where(event==24)#giving two indices of W jets\n",
    "\n",
    "#         #create array containing indices of particles in each event\n",
    "#         indices_particle_level = ak.concatenate([l_index, b_lep_index, b_top_index, w_index], axis=1)\n",
    "#         #create array of all events\n",
    "#         label_indices = ak.concatenate([label_indices, indices_particle_level], axis=0)\n",
    "\n",
    "#     return label_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label_indices from particles assignment labels\n",
    "l_indices, b_lep_indices, b_top_indices, w1_indices, w2_indices = label_indices(labels_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing sequential data for h5 file\n",
    "jets_data = {\n",
    "    'MASK': Jets_mask_padded, # capital MASK\n",
    "    'mass': Jets_mass_padded,\n",
    "    'pt': Jets_pt_padded,\n",
    "    'eta': Jets_eta_padded,\n",
    "    'phi': Jets_phi_padded,\n",
    "    'btag': Jets_btag_padded,\n",
    "    'qgl': Jets_qgl_padded,\n",
    "}\n",
    "\n",
    "lep_data = {\n",
    "    'MASK': lep_mask_padded, # capital MASK\n",
    "    'mass':lep_mass_padded,\n",
    "    'pt':lep_pt_padded,\n",
    "    'eta':lep_eta_padded,\n",
    "    'phi':lep_phi_padded\n",
    "}\n",
    "\n",
    "#preparing target data for h5 file\n",
    "ht_target = {\n",
    "    'b': b_top_indices,\n",
    "    'q1': w1_indices,\n",
    "    'q2': w2_indices,\n",
    "}\n",
    "\n",
    "lt_target = {\n",
    "    'b': b_lep_indices,\n",
    "    'l': l_indices,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating h5 file\n",
    "with h5py.File('testing_converted.h5', 'w') as h5file:\n",
    "    #INPUTS\n",
    "    input_group = h5file.create_group('INPUTS')\n",
    "#     seq_subgroup = input_group.create_group('SEQUENTIAL')\n",
    "#     global_subgroup = input_group.create_group('GLOBAL')\n",
    "\n",
    "#     jet_2subgroup = seq_subgroup.create_group('Jets')\n",
    "#     lept_2subgroup = seq_subgroup.create_group('Lepton')\n",
    "    jet_2subgroup = input_group.create_group('Jets')\n",
    "    lept_2subgroup = input_group.create_group('Lepton')\n",
    "\n",
    "    for key, value in jets_data.items():\n",
    "        jet_2subgroup.create_dataset(key, data=ak.to_numpy(value)) #cant do normal numpy convertion because of irregular awkward array\n",
    "\n",
    "    for key, value in lep_data.items():\n",
    "        lept_2subgroup.create_dataset(key, data=ak.to_numpy(value))\n",
    "\n",
    "    #EVENT\n",
    "    event_group = h5file.create_group('TARGETS')\n",
    "    ht_subgroup = event_group.create_group('ht')\n",
    "    lt_subgroup = event_group.create_group('lt')\n",
    "    \n",
    "    for key, value in ht_target.items():\n",
    "        ht_subgroup.create_dataset(key, data=ak.to_numpy(value))\n",
    "\n",
    "    for key, value in lt_target.items():\n",
    "        lt_subgroup.create_dataset(key, data=ak.to_numpy(value))\n",
    "\n",
    "\n",
    "    #PERMUTATION\n",
    "    perm_group = h5file.create_group('PERMUTATION')\n",
    "\n",
    "    #REGRESSION\n",
    "    regression_group = h5file.create_group('REGRESSION')\n",
    "\n",
    "    #CLASSIFICATION\n",
    "    classification_group = h5file.create_group('CLASSIFICATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from test.h5 file\n",
    "# testing reading h5 file\n",
    "filename_test = 'testing_converted.h5'\n",
    "f = h5py.File(filename_test, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['TARGETS']['ht']['b'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for training_converted.h5 \n",
      "============================================================\n",
      "\n",
      "|-CLASSIFICATION                \n",
      "|-INPUTS                        \n",
      "|---Jets                        \n",
      "|-----MASK                       :: bool     : (10736, 12)\n",
      "|-----btag                       :: float64  : (10736, 12)\n",
      "|-----eta                        :: float64  : (10736, 12)\n",
      "|-----mass                       :: float64  : (10736, 12)\n",
      "|-----phi                        :: float64  : (10736, 12)\n",
      "|-----pt                         :: float64  : (10736, 12)\n",
      "|-----qgl                        :: float64  : (10736, 12)\n",
      "|---Lepton                      \n",
      "|-----MASK                       :: bool     : (10736, 12)\n",
      "|-----eta                        :: float64  : (10736, 12)\n",
      "|-----mass                       :: float64  : (10736, 12)\n",
      "|-----phi                        :: float64  : (10736, 12)\n",
      "|-----pt                         :: float64  : (10736, 12)\n",
      "|-PERMUTATION                   \n",
      "|-REGRESSION                    \n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (10736,)\n",
      "|-----q1                         :: int64    : (10736,)\n",
      "|-----q2                         :: int64    : (10736,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (10736,)\n",
      "|-----l                          :: int64    : (10736,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py training_converted.h5 --shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SPANet Data/Model Construction (based on the demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_target_label(file_read):\n",
    "    lt_dict = {'l': [], 'b': []}\n",
    "    ht_dict = {'q1': [], 'q2': [], 'b': []}\n",
    "    \n",
    "    for key, value in lt_dict.items():\n",
    "        for label in file_read['TARGETS']['lt'][key][:]:\n",
    "            if label not in lt_dict[key]:\n",
    "                lt_dict[key].append(label)\n",
    "                \n",
    "    for key, value in ht_dict.items():\n",
    "        for label in file_read['TARGETS']['ht'][key][:]:\n",
    "            if label not in ht_dict[key]:\n",
    "                ht_dict[key].append(label)\n",
    "    \n",
    "    return lt_dict, ht_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud h5 files reading test\n",
    "train_filepath_cloud = '/eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation_subset.h5'\n",
    "test_filepath_cloud = '/eos/user/n/nmuangko/semi_leptonnic_ttbar/testing_mass_variation_subset.h5'\n",
    "# filepath_test = 'testing_mass_variation.h5' #path from SWAN\n",
    "# filepath_train = 'training_mass_variation.h5'\n",
    "train_file = h5py.File(train_filepath_cloud,'r')\n",
    "test_file = h5py.File(test_filepath_cloud,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KeysViewHDF5 ['INPUTS', 'REGRESSIONS', 'TARGETS']>,\n",
       " <KeysViewHDF5 ['INPUTS', 'REGRESSIONS', 'TARGETS']>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.keys(), test_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_side = train_file['TARGETS']['ht']\n",
    "lt_side = train_file['TARGETS']['lt']\n",
    "\n",
    "b_ht = ht_side['b']\n",
    "q1_ht = ht_side['q1']\n",
    "q2_ht = ht_side['q2']\n",
    "b_lt = lt_side['b']\n",
    "l_lt = lt_side['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadronic side::     b: 3  q1: 1  q2: 6\n",
      "Leptonic side::     b: 2  l: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "print('Hadronic side::   ', ' b:', b_ht[idx], ' q1:', q1_ht[idx], ' q2:', q2_ht[idx])\n",
    "print('Leptonic side::   ', ' b:', b_lt[idx], ' l:', l_lt[idx])\n",
    "# indices of lepton is always 0 because all event has one electron (need to indicate in .ymal so that lepton_target is associated with lepton_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, -1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file['TARGETS']['ht']['b'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 \n",
      "============================================================\n",
      "\n",
      "|-INPUTS                        \n",
      "|---Met                         \n",
      "|-----cos_phi                    :: float32  : (12427644,)\n",
      "|-----met                        :: float32  : (12427644,)\n",
      "|-----sin_phi                    :: float32  : (12427644,)\n",
      "|-----sumet                      :: float32  : (12427644,)\n",
      "|---Momenta                     \n",
      "|-----MASK                       :: bool     : (12427644, 16)\n",
      "|-----btag                       :: float32  : (12427644, 16)\n",
      "|-----cos_phi                    :: float32  : (12427644, 16)\n",
      "|-----eta                        :: float32  : (12427644, 16)\n",
      "|-----etag                       :: float32  : (12427644, 16)\n",
      "|-----mass                       :: float32  : (12427644, 16)\n",
      "|-----pt                         :: float32  : (12427644, 16)\n",
      "|-----qtag                       :: float32  : (12427644, 16)\n",
      "|-----rapidity                   :: float32  : (12427644, 16)\n",
      "|-----sin_phi                    :: float32  : (12427644, 16)\n",
      "|-----utag                       :: float32  : (12427644, 16)\n",
      "|-REGRESSIONS                   \n",
      "|---EVENT                       \n",
      "|-----invariant_mass             :: float32  : (12427644,)\n",
      "|-----log_invariant_mass         :: float32  : (12427644,)\n",
      "|-----log_neutrino_eta           :: float32  : (12427644,)\n",
      "|-----log_neutrino_px            :: float32  : (12427644,)\n",
      "|-----log_neutrino_py            :: float32  : (12427644,)\n",
      "|-----log_neutrino_pz            :: float32  : (12427644,)\n",
      "|-----mt                         :: float32  : (12427644,)\n",
      "|-----mx                         :: float32  : (12427644,)\n",
      "|-----neutrino_eta               :: float32  : (12427644,)\n",
      "|-----neutrino_px                :: float32  : (12427644,)\n",
      "|-----neutrino_py                :: float32  : (12427644,)\n",
      "|-----neutrino_pz                :: float32  : (12427644,)\n",
      "|---ht                          \n",
      "|-----PARTICLE                  \n",
      "|-------invariant_mass           :: float32  : (12427644,)\n",
      "|-------log_invariant_mass       :: float32  : (12427644,)\n",
      "|---lt                          \n",
      "|-----PARTICLE                  \n",
      "|-------invariant_mass           :: float32  : (12427644,)\n",
      "|-------log_invariant_mass       :: float32  : (12427644,)\n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (12427644,)\n",
      "|-----q1                         :: int64    : (12427644,)\n",
      "|-----q2                         :: int64    : (12427644,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (12427644,)\n",
      "|-----l                          :: int64    : (12427644,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 --shape\n",
    "# python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 --shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt_dict, ht_dict = check_target_label(f_test)\n",
    "\n",
    "# for key, value in lt_dict.items():\n",
    "#     print('Leptonic top side:', key, value)\n",
    "# for key, value in ht_dict.items():\n",
    "#     print('Hadronic top side:', key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### reducing testing and training size to avoid kernal crash\n",
    "# def copy_group(source_group, target_group, n_events_to_copy):\n",
    "#     for key in source_group.keys():\n",
    "#         item = source_group[key]\n",
    "#         if isinstance(item, h5py.Group):\n",
    "#             # Create the group in the target file\n",
    "#             print(f\"Creating group {key}\")\n",
    "#             new_group = target_group.create_group(key)\n",
    "#             # Recursively copy the subgroup\n",
    "#             copy_group(item, new_group, n_events_to_copy)\n",
    "#         elif isinstance(item, h5py.Dataset):\n",
    "#             print(f\"Copying dataset {key}\")\n",
    "#             # Copy the dataset with a subset of data\n",
    "#             if item.ndim == 1:\n",
    "#                 target_group.create_dataset(key, data=item[:n_events_to_copy])\n",
    "#             else:\n",
    "#                 target_group.create_dataset(key, data=item[:n_events_to_copy, :])\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown item type for {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newfile_path_test_subset = 'testing_mass_variation_subset_v2.h5'\n",
    "# newfile_path_train_subset = 'training_mass_variation_subset_v2.h5'\n",
    "\n",
    "# ratio = 0.8\n",
    "# test_num_events = int(ratio*(1865837))\n",
    "# train_num_events = int(ratio*(12427644))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the original HDF5 file\n",
    "# with h5py.File(train_filepath_cloud, 'r') as train_file:\n",
    "#     # Create a new HDF5 file\n",
    "#     with h5py.File(newfile_path_train_subset, 'w') as new_train_file:\n",
    "#         # Copy the structure and a subset of data\n",
    "#         copy_group(train_file, new_train_file, train_num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(test_filepath_cloud, 'r') as test_file:\n",
    "#     # Create a new HDF5 file\n",
    "#     with h5py.File(newfile_path_test_subset, 'w') as new_test_file:\n",
    "#         # Copy the structure and a subset of data\n",
    "#         copy_group(test_file, new_test_file, test_num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python utils/examine_hdf5.py training_mass_variation_subset_v2.h5 --shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
