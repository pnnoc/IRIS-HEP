{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot as ur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing root files, Events filtering, Labels assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_filter_multiple_jets(jets, electrons, muons, genparts, mets):\n",
    "    '''\n",
    "    Filters events down to training set and calculates jet-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after region filter (and selecting leading four for each event)\n",
    "        electrons: selected electrons after region filter\n",
    "        muons: selected muons after region filter\n",
    "        genparts: selected genpart after region filter\n",
    "        even: whether the event is even-numbered (used to separate training events)\n",
    "    \n",
    "    Returns:\n",
    "        jets: selected jets after training filter\n",
    "        electrons: selected electrons after training filter\n",
    "        muons: selected muons after training filter\n",
    "        labels: labels of jets within an event (24=W, 6=top_hadron, -6=top_lepton)\n",
    "        even: whether the event is even-numbered\n",
    "    '''\n",
    "    #### filter genPart to valid matching candidates ####\n",
    "\n",
    "    # get rid of particles without parents\n",
    "    genpart_parent = genparts.distinctParent\n",
    "    genpart_filter = np.invert(ak.is_none(genpart_parent, axis=1))\n",
    "    genparts = genparts[genpart_filter]\n",
    "    genpart_parent = genparts.distinctParent\n",
    "\n",
    "    # ensure that parents are top quark or W\n",
    "    genpart_filter2 = ((np.abs(genpart_parent.pdgId)==6) | (np.abs(genpart_parent.pdgId)==24))\n",
    "    genparts = genparts[genpart_filter2]\n",
    "\n",
    "    # ensure particle itself is a quark\n",
    "    genpart_filter3 = ((np.abs(genparts.pdgId)<7) & (np.abs(genparts.pdgId)>0))\n",
    "    genparts = genparts[genpart_filter3]\n",
    "\n",
    "    # get rid of duplicates\n",
    "    genpart_filter4 = genparts.hasFlags(\"isLastCopy\")\n",
    "    genparts = genparts[genpart_filter4]\n",
    "            \n",
    "        \n",
    "    #### get jet-level labels and filter events to training set\n",
    "        \n",
    "    # match jets to nearest valid genPart candidate\n",
    "    nearest_genpart = jets.nearest(genparts, threshold=0.4)\n",
    "    nearest_parent = nearest_genpart.distinctParent # parent of matched particle\n",
    "    parent_pdgid = nearest_parent.pdgId # pdgId of parent particle\n",
    "    grandchild_pdgid = nearest_parent.distinctChildren.distinctChildren.pdgId # pdgId of particle's parent's grandchildren\n",
    "\n",
    "    grandchildren_flat = np.abs(ak.flatten(grandchild_pdgid,axis=-1)) # flatten innermost axis for convenience\n",
    "\n",
    "    # if particle has a cousin that is a lepton\n",
    "    has_lepton_cousin = (ak.sum(((grandchildren_flat%2==0) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                axis=-1)>0)\n",
    "    # if particle has a cousin that is a neutrino\n",
    "    has_neutrino_cousin = (ak.sum(((grandchildren_flat%2==1) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                  axis=-1)>0)\n",
    "\n",
    "    # if a particle has a lepton cousin and a neutrino cousin\n",
    "    has_both_cousins = ak.fill_none((has_lepton_cousin & has_neutrino_cousin), False) #not using .to_numpy bc inregular array size (different event level, multiple jets)\n",
    "\n",
    "    # get labels from parent pdgId (fill none with 100 to filter out events with those jets)\n",
    "    labels = np.abs(ak.fill_none(parent_pdgid,100)) #not using .to_numpy bc inregular array size (different event level, multiple jets)\n",
    "\n",
    "    # changing the labels while still preserve awkward array jets. To bypass inplace assignment error of numpy_array vs awkward_array\n",
    "    new_labels = ak.Array([])\n",
    "    for idx in range(len(labels)):\n",
    "        (labels[idx].to_numpy())[has_both_cousins[idx].to_numpy()]=-6\n",
    "        new_labels = ak.concatenate([new_labels, ak.Array([labels[idx]])], axis=0)\n",
    "    labels = new_labels\n",
    "    \n",
    "    #mask for event validation (atleast 4 jets with -6, 6, 24, 24, 100, 100,...)\n",
    "    mask = ak.Array([])\n",
    "    for idx in range(len(labels)):\n",
    "        event_valid_bool = (ak.sum(labels[idx]==-6)==1) & (ak.sum(labels[idx]==6)==1) & (ak.sum(labels[idx]==24)==2)\n",
    "        mask = ak.concatenate([mask, ak.Array([event_valid_bool])], axis=0)\n",
    "            \n",
    "    # filter events\n",
    "    jets = jets[mask]\n",
    "    electrons = electrons[mask]\n",
    "    muons = muons[mask]\n",
    "    labels = labels[mask]\n",
    "    mets = mets[mask]\n",
    "    \n",
    "    return jets, electrons, muons, mets, labels\n",
    "\n",
    "# def pad_array(array, max_jets=13, pad_value=0): #padded array for features, except MASK\n",
    "#     padded_array = ak.pad_none(array, max_jets, axis=1)\n",
    "#     padded_array_filled = ak.fill_none(padded_array, pad_value, axis=1)\n",
    "#     return padded_array_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_20000_0007.root'\n",
    "tree_name = 'Events'\n",
    "events = NanoEventsFactory.from_root(file_path, treepath=tree_name, schemaclass=NanoAODSchema).events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPANet Event Filtering\n",
    "# electrons = events.Electron\n",
    "# muons = events.Muon\n",
    "# jets = events.Jet\n",
    "\n",
    "selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta)<2.1) & \n",
    "                                        (events.Electron.cutBased==4) & (events.Electron.sip3d < 4)]\n",
    "selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta)<2.1) & (events.Muon.tightId) & \n",
    "                                (events.Muon.sip3d < 4) & (events.Muon.pfRelIso04_all < 0.15)]\n",
    "jet_filter = (events.Jet.pt > 30) & (np.abs(events.Jet.eta) < 2.4) & (events.Jet.isTightLeptonVeto)\n",
    "\n",
    "# selected_electrons = electrons[(electrons.pt > 30) & (np.abs(electrons.eta)<2.1) & \n",
    "#                                         (electrons.cutBased==4) & (electrons.sip3d < 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_muons = muons[(muons.pt > 30) & (np.abs(muons.eta)<2.1) & (muons.tightId) & \n",
    "#                                 (muons.sip3d < 4) & (muons.pfRelIso04_all < 0.15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet_filter = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_jets = events.Jet[jet_filter]\n",
    "selected_genpart = events.GenPart\n",
    "selected_MET = events.MET # Part of SPANet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lepton requirement\n",
    "event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "# require at least 4 jets\n",
    "event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "# require at least one jet above B_TAG_THRESHOLD\n",
    "B_TAG_THRESHOLD = 0.5\n",
    "event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "# apply event filters\n",
    "selected_electrons = selected_electrons[event_filters]\n",
    "selected_muons = selected_muons[event_filters]\n",
    "selected_jets = selected_jets[event_filters]\n",
    "selected_genpart = selected_genpart[event_filters]\n",
    "selected_MET = selected_MET[event_filters]\n",
    "\n",
    "### only consider 4j2b (signal) region\n",
    "region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2 # at least two b-tagged jets\n",
    "selected_jets_region = selected_jets[region_filter] # all jet\n",
    "selected_electrons_region = selected_electrons[region_filter]\n",
    "selected_muons_region = selected_muons[region_filter]\n",
    "selected_genpart_region = selected_genpart[region_filter]\n",
    "selected_MET_region = selected_MET[region_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting labels for valid event\n",
    "jets, electrons, muons, mets, labels = training_filter_multiple_jets(selected_jets_region, selected_electrons_region, selected_muons_region, selected_genpart_region, selected_MET_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(max(ak.num(jets, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jets\n",
    "jets_btag = jets.btagCSVV2 #awkward array (not padded yet)\n",
    "jets_mass = jets.mass\n",
    "jets_pt = jets.pt\n",
    "jets_eta = jets.eta\n",
    "jets_sin_phi = np.sin(jets.phi)\n",
    "jets_cos_phi = np.cos(jets.phi)\n",
    "\n",
    "#Met\n",
    "mets_sumEt = mets.sumEt #awkward array (no padded needed)\n",
    "mets_sin_phi = np.sin(mets.phi)\n",
    "mets_cos_phi = np.cos(mets.phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events = ak.num(jets, axis=0)\n",
    "\n",
    "#initializing (for memory allocation)\n",
    "leptons_btag = np.zeros(num_events)\n",
    "leptons_mass = np.zeros(num_events)\n",
    "leptons_pt = np.zeros(num_events)\n",
    "leptons_eta = np.zeros(num_events)\n",
    "leptons_sin_phi = np.zeros(num_events)\n",
    "leptons_cos_phi = np.zeros(num_events)\n",
    "\n",
    "# creating lepton mask (either one electron or one muon)\n",
    "has_electron = ak.num(electrons, axis=1) > 0\n",
    "has_muon = ak.num(muons, axis=1) > 0\n",
    "\n",
    "#filling electrons features from events\n",
    "leptons_mass[has_electron] = electrons.mass[has_electron][:, 0]\n",
    "leptons_pt[has_electron] = electrons.pt[has_electron][:, 0]\n",
    "leptons_eta[has_electron] = electrons.eta[has_electron][:, 0]\n",
    "leptons_sin_phi[has_electron] = np.sin(electrons.phi[has_electron][:, 0])\n",
    "leptons_cos_phi[has_electron] = np.cos(electrons.phi[has_electron][:, 0])\n",
    "#fillinig  muons features from events\n",
    "leptons_mass[has_muon] = muons.mass[has_muon][:, 0]\n",
    "leptons_pt[has_muon] = muons.pt[has_muon][:, 0]\n",
    "leptons_eta[has_muon] = muons.eta[has_muon][:, 0]\n",
    "leptons_sin_phi[has_muon] = np.sin(muons.phi[has_muon][:, 0])\n",
    "leptons_cos_phi[has_muon] = np.cos(muons.phi[has_muon][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining momenta (leptons + jets)\n",
    "# max_num_jets = ak.max(ak.num(jets, axis=1))\n",
    "max_num_jets = 13\n",
    "\n",
    "momenta_btag = np.zeros((num_events, max_num_jets + 1)) #initializing 2d array. num_events*(max_num_jets+1 lepton) (padded)\n",
    "momenta_mass = np.zeros((num_events, max_num_jets + 1))\n",
    "momenta_pt = np.zeros((num_events, max_num_jets + 1))\n",
    "momenta_eta = np.zeros((num_events, max_num_jets + 1))\n",
    "momenta_sin_phi = np.zeros((num_events, max_num_jets + 1))\n",
    "momenta_cos_phi = np.zeros((num_events, max_num_jets + 1))\n",
    "\n",
    "#the 0th argument from lepton information\n",
    "momenta_btag[:, 0] = leptons_btag #no btag for electron. Thus, all zero.\n",
    "momenta_mass[:, 0] = leptons_mass\n",
    "momenta_pt[:, 0] = leptons_pt\n",
    "momenta_eta[:, 0] = leptons_eta\n",
    "momenta_sin_phi[:, 0] = leptons_sin_phi\n",
    "momenta_cos_phi[:, 0] = leptons_cos_phi\n",
    "\n",
    "#filling jets into momenta\n",
    "for idx in range(max_num_jets): #going through jets level\n",
    "    mask = ak.num(jets, axis=1) > idx #filtering events that has num_jets > idx\n",
    "    momenta_btag[mask, idx + 1] = jets_btag[mask][:, idx]\n",
    "    momenta_mass[mask, idx + 1] = jets_mass[mask][:, idx]\n",
    "    momenta_pt[mask, idx + 1] = jets_pt[mask][:, idx]\n",
    "    momenta_eta[mask, idx + 1] = jets_eta[mask][:, idx]\n",
    "    momenta_sin_phi[mask, idx + 1] = jets_sin_phi[mask][:, idx]\n",
    "    momenta_cos_phi[mask, idx + 1] = jets_cos_phi[mask][:, idx]\n",
    "\n",
    "momenta_MASK = momenta_mass != 0 #this should valid (since mass would never be zero -> jets/leptons exist)\n",
    "\n",
    "#convert met to numpy\n",
    "mets_sumEt = mets_sumEt.to_numpy()\n",
    "mets_sin_phi = mets_sin_phi.to_numpy()\n",
    "mets_cos_phi = mets_cos_phi.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAGET indices ###\n",
    "#ht\n",
    "b_had_indices = np.zeros(num_events, dtype=int)\n",
    "q1_indices = np.zeros(num_events, dtype=int)\n",
    "q2_indices = np.zeros(num_events, dtype=int)\n",
    "#lt\n",
    "b_lep_indices = np.zeros(num_events, dtype=int)\n",
    "l_indices = np.zeros(num_events, dtype=int)\n",
    "\n",
    "b_lep_indices[:] = ak.argmax(labels == -6, axis=1) + 1 #avoiding for-loop since there is exact one -6 and one 6\n",
    "b_had_indices[:] = ak.argmax(labels == 6, axis=1) + 1\n",
    "  \n",
    "# Find indices for 24\n",
    "for idx, event in enumerate(labels):\n",
    "    q_indices = ak.where(event == 24)[0]\n",
    "    q1_indices[idx] = q_indices[0] + 1\n",
    "    q2_indices[idx] = q_indices[1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing data for h5 file ###\n",
    "#INPUTS\n",
    "##Momenta\n",
    "momenta_data = {\n",
    "    'MASK': momenta_MASK, # capital MASK\n",
    "    'mass': momenta_mass,\n",
    "    'pt': momenta_pt,\n",
    "    'eta': momenta_eta,\n",
    "    'cos_phi': momenta_cos_phi,\n",
    "    'sin_phi': momenta_sin_phi,\n",
    "    'btag': momenta_btag,\n",
    "}\n",
    "##Met\n",
    "met_data = {\n",
    "    'sumet': mets_sumEt,\n",
    "    'cos_phi': mets_cos_phi,\n",
    "    'sin_phi': mets_sin_phi,\n",
    "}\n",
    "\n",
    "\n",
    "#TARGETS\n",
    "ht_target = {\n",
    "    'b': b_had_indices,\n",
    "    'q1': q1_indices,\n",
    "    'q2': q2_indices,\n",
    "}\n",
    "\n",
    "lt_target = {\n",
    "    'b': b_lep_indices,\n",
    "    'l': l_indices,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4841, 14)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momenta_data['mass'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating h5 file\n",
    "with h5py.File('training_SPANet_structure_ext3-v1_20000_0008.h5', 'w') as h5file:\n",
    "    #INPUTS\n",
    "    input_group = h5file.create_group('INPUTS')\n",
    "    momenta_subgroup = input_group.create_group('Momenta')\n",
    "    met_subgroup = input_group.create_group('Met')\n",
    "\n",
    "    for key, value in momenta_data.items():\n",
    "        momenta_subgroup.create_dataset(key, data=value)\n",
    "    for key, value in met_data.items():\n",
    "        met_subgroup.create_dataset(key, data=value)\n",
    "\n",
    "    #EVENT\n",
    "    event_group = h5file.create_group('TARGETS')\n",
    "    ht_subgroup = event_group.create_group('ht')\n",
    "    lt_subgroup = event_group.create_group('lt')\n",
    "    \n",
    "    for key, value in ht_target.items():\n",
    "        ht_subgroup.create_dataset(key, data=value)\n",
    "\n",
    "    for key, value in lt_target.items():\n",
    "        lt_subgroup.create_dataset(key, data=value)\n",
    "\n",
    "    #PERMUTATION\n",
    "    perm_group = h5file.create_group('PERMUTATION')\n",
    "\n",
    "    #REGRESSION\n",
    "    regression_group = h5file.create_group('REGRESSION')\n",
    "\n",
    "    #CLASSIFICATION\n",
    "    classification_group = h5file.create_group('CLASSIFICATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPANet Demo Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUTS\n",
    "##Momenta\n",
    "\n",
    "##Met\n",
    "\n",
    "#TARGETS\n",
    "##ht\n",
    "\n",
    "\n",
    "##lt\n",
    "\n",
    "\n",
    "#REGRESSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating padded features for each event\n",
    "# Jets_pt_padded = pad_array(jets.pt)\n",
    "# Jets_mass_padded = pad_array(jets.mass)\n",
    "# Jets_eta_padded = pad_array(jets.eta)\n",
    "# Jets_phi_padded = pad_array(jets.phi)\n",
    "# Jets_btag_padded = pad_array(jets.btagCSVV2)\n",
    "# Jets_qgl_padded = pad_array(jets.qgl)\n",
    "\n",
    "# #lepton features (only electrons left)\n",
    "# #Since there's only one lepton, might not need to pad\n",
    "# lep_pt_padded = pad_array(electrons.pt)\n",
    "# lep_mass_padded = pad_array(electrons.mass)\n",
    "# lep_eta_padded = pad_array(electrons.eta)\n",
    "# lep_phi_padded = pad_array(electrons.phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating MASK for each Jet event\n",
    "# pad_check = ak.pad_none(jets, 12, axis=1)\n",
    "# Jets_mask_padded = np.invert(ak.is_none(pad_check, axis=1))\n",
    "\n",
    "# #creating MASK for each Lepton event\n",
    "# lep_mask_padded = np.invert(ak.is_none(pad_check, axis=1)) #not sure if this is necessary\n",
    "\n",
    "# #labels padded (missing jets should be label as -1 ?) #probably dont need padded_labels to preserve memory (only need to extract indices)\n",
    "# labels_padded = pad_array(labels, pad_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #target indices fot jets and lepton\n",
    "# def label_indices(labels):\n",
    "#     l_indices = ak.Array([])\n",
    "#     b_lep_indices = ak.Array([])\n",
    "#     b_top_indices = ak.Array([])\n",
    "#     w1_indices = ak.Array([])\n",
    "#     w2_indices = ak.Array([])\n",
    "    \n",
    "    \n",
    "#     for event in labels:\n",
    "#         l_index = ak.Array([0]) #always 0th because only 1 lepton(electron) in each event. lepton target associated with lepton sequential input\n",
    "#         b_lep_index = ak.where(event==-6) #b_top_lep\n",
    "#         b_top_index = ak.where(event==6) #b_top_had\n",
    "#         w_index = ak.where(event==24)#giving two indices of W jets\n",
    "        \n",
    "#         l_indices = ak.concatenate([l_indices, l_index], axis=0) # worked (alreay flatten)\n",
    "#         b_lep_indices = ak.concatenate([b_lep_indices, b_lep_index], axis=0)\n",
    "#         b_top_indices = ak.concatenate([b_top_indices, b_top_index], axis=0)\n",
    "        \n",
    "#         w1_indices = ak.concatenate([w1_indices, ak.Array([w_index[0][0]])], axis=0) #worked (alreay flatten)\n",
    "#         w2_indices = ak.concatenate([w2_indices, ak.Array([w_index[0][1]])], axis=0) #worked (alreay flatten)\n",
    "    \n",
    "#     b_lep_indices = ak.flatten(b_lep_indices)\n",
    "#     b_top_indices = ak.flatten(b_top_indices)\n",
    "        \n",
    "#     return l_indices, b_lep_indices, b_top_indices, w1_indices, w2_indices\n",
    "       \n",
    "# #     labels_indices = ak.Array([])\n",
    "    \n",
    "# #     for event in labels:\n",
    "# #         l_index = ak.Array([[0]]) #always 0th because only 1 lepton(electron) in each event. lepton target associated with lepton sequential input\n",
    "# #         b_lep_index = ak.where(event==-6) #b_top_lep\n",
    "# #         b_top_index = ak.where(event==6) #b_top_had\n",
    "# #         w_index = ak.where(event==24)#giving two indices of W jets\n",
    "\n",
    "# #         #create array containing indices of particles in each event\n",
    "# #         indices_particle_level = ak.concatenate([l_index, b_lep_index, b_top_index, w_index], axis=1)\n",
    "# #         #create array of all events\n",
    "# #         label_indices = ak.concatenate([label_indices, indices_particle_level], axis=0)\n",
    "\n",
    "# #     return label_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create label_indices from particles assignment labels\n",
    "# l_indices, b_lep_indices, b_top_indices, w1_indices, w2_indices = label_indices(labels_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_lep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preparing sequential data for h5 file\n",
    "# jets_data = {\n",
    "#     'MASK': Jets_mask_padded, # capital MASK\n",
    "#     'mass': Jets_mass_padded,\n",
    "#     'pt': Jets_pt_padded,\n",
    "#     'eta': Jets_eta_padded,\n",
    "#     'phi': Jets_phi_padded,\n",
    "#     'btag': Jets_btag_padded,\n",
    "#     'qgl': Jets_qgl_padded,\n",
    "# }\n",
    "\n",
    "# lep_data = {\n",
    "#     'MASK': lep_mask_padded, # capital MASK\n",
    "#     'mass':lep_mass_padded,\n",
    "#     'pt':lep_pt_padded,\n",
    "#     'eta':lep_eta_padded,\n",
    "#     'phi':lep_phi_padded\n",
    "# }\n",
    "\n",
    "# #preparing target data for h5 file\n",
    "# ht_target = {\n",
    "#     'b': b_top_indices,\n",
    "#     'q1': w1_indices,\n",
    "#     'q2': w2_indices,\n",
    "# }\n",
    "\n",
    "# lt_target = {\n",
    "#     'b': b_lep_indices,\n",
    "#     'l': l_indices,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from test.h5 file\n",
    "# testing reading h5 file\n",
    "filename_test = 'training_converted.h5'\n",
    "f = h5py.File(filename_test, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['TARGETS']['ht']['b'][0], f['TARGETS']['ht']['q1'][0], f['TARGETS']['ht']['q2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['TARGETS']['lt']['b'][1], f['TARGETS']['lt']['l'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py training_SPANet_structure_ext3-v1_00000_0003.h5 --shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SPANet Data/Model Construction (based on the demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud h5 files reading test\n",
    "train_filepath_cloud = '/eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation_subset.h5'\n",
    "test_filepath_cloud = '/eos/user/n/nmuangko/semi_leptonnic_ttbar/testing_mass_variation_subset.h5'\n",
    "# filepath_test = 'testing_mass_variation.h5' #path from SWAN\n",
    "# filepath_train = 'training_mass_variation.h5'\n",
    "train_file = h5py.File(train_filepath_cloud,'r')\n",
    "test_file = h5py.File(test_filepath_cloud,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.keys(), test_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['INPUTS']['Momenta']['MASK'][:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_side = train_file['TARGETS']['ht']\n",
    "lt_side = train_file['TARGETS']['lt']\n",
    "\n",
    "b_ht = ht_side['b']\n",
    "q1_ht = ht_side['q1']\n",
    "q2_ht = ht_side['q2']\n",
    "b_lt = lt_side['b']\n",
    "l_lt = lt_side['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "print('Hadronic side::   ', ' b:', b_ht[idx], ' q1:', q1_ht[idx], ' q2:', q2_ht[idx])\n",
    "print('Leptonic side::   ', ' b:', b_lt[idx], ' l:', l_lt[idx])\n",
    "# indices of lepton is always 0 because all event has one electron (need to indicate in .ymal so that lepton_target is associated with lepton_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['TARGETS']['ht']['b'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for /eos/user/n/nmuangko/semi_leptonnic_ttbar/events_h5_files/testing_SPANet_02.h5 \n",
      "============================================================\n",
      "\n",
      "|-CLASSIFICATION                \n",
      "|-INPUTS                        \n",
      "|---Met                         \n",
      "|-----cos_phi                    :: float32  : (440691,)\n",
      "|-----sin_phi                    :: float32  : (440691,)\n",
      "|-----sumet                      :: float32  : (440691,)\n",
      "|---Momenta                     \n",
      "|-----MASK                       :: bool     : (440691, 17)\n",
      "|-----btag                       :: float64  : (440691, 17)\n",
      "|-----cos_phi                    :: float64  : (440691, 17)\n",
      "|-----eta                        :: float64  : (440691, 17)\n",
      "|-----mass                       :: float64  : (440691, 17)\n",
      "|-----pt                         :: float64  : (440691, 17)\n",
      "|-----sin_phi                    :: float64  : (440691, 17)\n",
      "|-PERMUTATION                   \n",
      "|-REGRESSION                    \n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (440691,)\n",
      "|-----q1                         :: int64    : (440691,)\n",
      "|-----q2                         :: int64    : (440691,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (440691,)\n",
      "|-----l                          :: int64    : (440691,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/events_h5_files/testing_SPANet_02.h5 --shape\n",
    "# python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 --shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt_dict, ht_dict = check_target_label(f_test)\n",
    "\n",
    "# for key, value in lt_dict.items():\n",
    "#     print('Leptonic top side:', key, value)\n",
    "# for key, value in ht_dict.items():\n",
    "#     print('Hadronic top side:', key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### reducing testing and training size to avoid kernal crash\n",
    "# def copy_group(source_group, target_group, n_events_to_copy):\n",
    "#     for key in source_group.keys():\n",
    "#         item = source_group[key]\n",
    "#         if isinstance(item, h5py.Group):\n",
    "#             # Create the group in the target file\n",
    "#             print(f\"Creating group {key}\")\n",
    "#             new_group = target_group.create_group(key)\n",
    "#             # Recursively copy the subgroup\n",
    "#             copy_group(item, new_group, n_events_to_copy)\n",
    "#         elif isinstance(item, h5py.Dataset):\n",
    "#             print(f\"Copying dataset {key}\")\n",
    "#             # Copy the dataset with a subset of data\n",
    "#             if item.ndim == 1:\n",
    "#                 target_group.create_dataset(key, data=item[:n_events_to_copy])\n",
    "#             else:\n",
    "#                 target_group.create_dataset(key, data=item[:n_events_to_copy, :])\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown item type for {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newfile_path_test_subset = 'testing_mass_variation_subset_v2.h5'\n",
    "# newfile_path_train_subset = 'training_mass_variation_subset_v2.h5'\n",
    "\n",
    "# ratio = 0.8\n",
    "# test_num_events = int(ratio*(1865837))\n",
    "# train_num_events = int(ratio*(12427644))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the original HDF5 file\n",
    "# with h5py.File(train_filepath_cloud, 'r') as train_file:\n",
    "#     # Create a new HDF5 file\n",
    "#     with h5py.File(newfile_path_train_subset, 'w') as new_train_file:\n",
    "#         # Copy the structure and a subset of data\n",
    "#         copy_group(train_file, new_train_file, train_num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(test_filepath_cloud, 'r') as test_file:\n",
    "#     # Create a new HDF5 file\n",
    "#     with h5py.File(newfile_path_test_subset, 'w') as new_test_file:\n",
    "#         # Copy the structure and a subset of data\n",
    "#         copy_group(test_file, new_test_file, test_num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for training_SPANet_structure_ext3-v1_00000_0002.h5 \n",
      "============================================================\n",
      "\n",
      "|-CLASSIFICATION                \n",
      "|-INPUTS                        \n",
      "|---Met                         \n",
      "|-----cos_phi                    :: float32  : (10509,)\n",
      "|-----sin_phi                    :: float32  : (10509,)\n",
      "|-----sumet                      :: float32  : (10509,)\n",
      "|---Momenta                     \n",
      "|-----MASK                       :: bool     : (10509, 14)\n",
      "|-----btag                       :: float64  : (10509, 14)\n",
      "|-----cos_phi                    :: float64  : (10509, 14)\n",
      "|-----eta                        :: float64  : (10509, 14)\n",
      "|-----mass                       :: float64  : (10509, 14)\n",
      "|-----pt                         :: float64  : (10509, 14)\n",
      "|-----sin_phi                    :: float64  : (10509, 14)\n",
      "|-PERMUTATION                   \n",
      "|-REGRESSION                    \n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (10509,)\n",
      "|-----q1                         :: int64    : (10509,)\n",
      "|-----q2                         :: int64    : (10509,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (10509,)\n",
      "|-----l                          :: int64    : (10509,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py training_SPANet_structure_ext3-v1_00000_0002.h5 --shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
