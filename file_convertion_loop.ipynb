{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: unordered jets\n",
    "Outpu: one head per particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot as ur\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing root files, Events filtering, Labels assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_filter_multiple_jets(jets, electrons, muons, genparts, mets):\n",
    "    '''\n",
    "    Filters events down to training set and calculates jet-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after region filter (and selecting leading four for each event)\n",
    "        electrons: selected electrons after region filter\n",
    "        muons: selected muons after region filter\n",
    "        genparts: selected genpart after region filter\n",
    "        even: whether the event is even-numbered (used to separate training events)\n",
    "    \n",
    "    Returns:\n",
    "        jets: selected jets after training filter\n",
    "        electrons: selected electrons after training filter\n",
    "        muons: selected muons after training filter\n",
    "        labels: labels of jets within an event (24=W, 6=top_hadron, -6=top_lepton)\n",
    "        even: whether the event is even-numbered\n",
    "    '''\n",
    "    #### filter genPart to valid matching candidates ####\n",
    "\n",
    "    # get rid of particles without parents\n",
    "    genpart_parent = genparts.distinctParent\n",
    "    genpart_filter = np.invert(ak.is_none(genpart_parent, axis=1))\n",
    "    genparts = genparts[genpart_filter]\n",
    "    genpart_parent = genparts.distinctParent\n",
    "\n",
    "    # ensure that parents are top quark or W\n",
    "    genpart_filter2 = ((np.abs(genpart_parent.pdgId)==6) | (np.abs(genpart_parent.pdgId)==24))\n",
    "    genparts = genparts[genpart_filter2]\n",
    "\n",
    "    # ensure particle itself is a quark\n",
    "    genpart_filter3 = ((np.abs(genparts.pdgId)<7) & (np.abs(genparts.pdgId)>0))\n",
    "    genparts = genparts[genpart_filter3]\n",
    "\n",
    "    # get rid of duplicates\n",
    "    genpart_filter4 = genparts.hasFlags(\"isLastCopy\")\n",
    "    genparts = genparts[genpart_filter4]\n",
    "            \n",
    "        \n",
    "    #### get jet-level labels and filter events to training set\n",
    "        \n",
    "    # match jets to nearest valid genPart candidate\n",
    "    nearest_genpart = jets.nearest(genparts, threshold=0.4)\n",
    "    nearest_parent = nearest_genpart.distinctParent # parent of matched particle\n",
    "    parent_pdgid = nearest_parent.pdgId # pdgId of parent particle\n",
    "    grandchild_pdgid = nearest_parent.distinctChildren.distinctChildren.pdgId # pdgId of particle's parent's grandchildren\n",
    "\n",
    "    grandchildren_flat = np.abs(ak.flatten(grandchild_pdgid,axis=-1)) # flatten innermost axis for convenience\n",
    "\n",
    "    # if particle has a cousin that is a lepton\n",
    "    has_lepton_cousin = (ak.sum(((grandchildren_flat%2==0) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                axis=-1)>0)\n",
    "    # if particle has a cousin that is a neutrino\n",
    "    has_neutrino_cousin = (ak.sum(((grandchildren_flat%2==1) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                  axis=-1)>0)\n",
    "\n",
    "    # if a particle has a lepton cousin and a neutrino cousin\n",
    "    has_both_cousins = ak.fill_none((has_lepton_cousin & has_neutrino_cousin), False) #not using .to_numpy bc inregular array size (different event level, multiple jets)\n",
    "\n",
    "    # get labels from parent pdgId (fill none with 100 to filter out events with those jets)\n",
    "    labels = np.abs(ak.fill_none(parent_pdgid,100)) #not using .to_numpy bc inregular array size (different event level, multiple jets)\n",
    "\n",
    "    # changing the labels while still preserve awkward array jets. To bypass inplace assignment error of numpy_array vs awkward_array\n",
    "    new_labels = ak.Array([])\n",
    "    for idx in range(len(labels)):\n",
    "        (labels[idx].to_numpy())[has_both_cousins[idx].to_numpy()]=-6\n",
    "        new_labels = ak.concatenate([new_labels, ak.Array([labels[idx]])], axis=0)\n",
    "    labels = new_labels\n",
    "    \n",
    "    #mask for event validation (atleast 4 jets with -6, 6, 24, 24, 100, 100,...)\n",
    "    mask = ak.Array([])\n",
    "    for idx in range(len(labels)):\n",
    "        event_valid_bool = (ak.sum(labels[idx]==-6)==1) & (ak.sum(labels[idx]==6)==1) & (ak.sum(labels[idx]==24)==2)\n",
    "        mask = ak.concatenate([mask, ak.Array([event_valid_bool])], axis=0)\n",
    "            \n",
    "    # filter events\n",
    "    jets = jets[mask]\n",
    "    electrons = electrons[mask]\n",
    "    muons = muons[mask]\n",
    "    labels = labels[mask]\n",
    "    mets = mets[mask]\n",
    "    \n",
    "    return jets, electrons, muons, mets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"nanoaod_inputs.json\") as f:\n",
    "#     file_info = json.load(f)\n",
    "\n",
    "# for file in file_info['ttbar']['scaledown']['files']:\n",
    "#     input_file = file['path']\n",
    "#     print(input_file)\n",
    "#     suffix = file['path'].replace('.root','.h5').split('_')[-3:]\n",
    "#     file_suffix = '_'.join(suffix)\n",
    "#     output_file = 'scaledown_SPANet_structure' + '_' + file_suffix\n",
    "#     print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalup_SPANet_structure_ext3-v1_10000_0000.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0001.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0002.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0003.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0004.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0005.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0006.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0007.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0008.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0009.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0010.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0011.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0012.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0013.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0014.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0015.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0016.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0017.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0018.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0019.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0020.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_10000_0021.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_40000_0000.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_40000_0001.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_40000_0002.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_40000_0003.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_40000_0004.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_40000_0005.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_50000_0000.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_50000_0001.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_50000_0002.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_50000_0003.h5 : convertion done\n",
      "scalup_SPANet_structure_ext3-v1_80000_0000.h5 : convertion done\n"
     ]
    }
   ],
   "source": [
    "# list of files\n",
    "with open(\"nanoaod_inputs.json\") as f:\n",
    "    file_info = json.load(f)\n",
    "\n",
    "for file in file_info['ttbar']['scaleup']['files']:\n",
    "    input_file = file['path']\n",
    "#     print(input_file)\n",
    "    suffix = file['path'].replace('.root','.h5').split('_')[-3:]\n",
    "    file_suffix = '_'.join(suffix)\n",
    "    output_file = 'scalup_SPANet_structure' + '_' + file_suffix\n",
    "#     print(output_file)\n",
    "\n",
    "# for file in file_list:\n",
    "#     input_file = file\n",
    "# #     print(file)\n",
    "#     suffix = file.replace('.root','.h5').split('_')[-3:]\n",
    "#     file_suffix = '_'.join(suffix)\n",
    "#     output_file = 'training_SPANet_structure' + '_' + file_suffix\n",
    "# #     print(output_file)\n",
    "\n",
    "    tree_name = 'Events'\n",
    "    events = NanoEventsFactory.from_root(input_file, treepath=tree_name, schemaclass=NanoAODSchema).events()\n",
    "    \n",
    "    ### SPANet Event Filtering\n",
    "    selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta)<2.1) & \n",
    "                                            (events.Electron.cutBased==4) & (events.Electron.sip3d < 4)]\n",
    "    selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta)<2.1) & (events.Muon.tightId) & \n",
    "                                    (events.Muon.sip3d < 4) & (events.Muon.pfRelIso04_all < 0.15)]\n",
    "    jet_filter = (events.Jet.pt > 30) & (np.abs(events.Jet.eta) < 2.4) & (events.Jet.isTightLeptonVeto)\n",
    "    selected_jets = events.Jet[jet_filter]\n",
    "    selected_genpart = events.GenPart\n",
    "    selected_MET = events.MET # Part of SPANet features\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # require at least 4 jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # require at least one jet above B_TAG_THRESHOLD\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "    # apply event filters\n",
    "    selected_electrons = selected_electrons[event_filters]\n",
    "    selected_muons = selected_muons[event_filters]\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "    selected_genpart = selected_genpart[event_filters]\n",
    "    selected_MET = selected_MET[event_filters]\n",
    "\n",
    "    ### only consider 4j2b (signal) region\n",
    "    region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2 # at least two b-tagged jets\n",
    "    selected_jets_region = selected_jets[region_filter] # all jet\n",
    "    selected_electrons_region = selected_electrons[region_filter]\n",
    "    selected_muons_region = selected_muons[region_filter]\n",
    "    selected_genpart_region = selected_genpart[region_filter]\n",
    "    selected_MET_region = selected_MET[region_filter]\n",
    "    \n",
    "    #getting labels for valid event\n",
    "    jets, electrons, muons, mets, labels = training_filter_multiple_jets(selected_jets_region, selected_electrons_region, selected_muons_region, selected_genpart_region, selected_MET_region)\n",
    "    \n",
    "\n",
    "    #jets\n",
    "    jets_btag = jets.btagCSVV2 #awkward array (not padded yet)\n",
    "    jets_mass = jets.mass\n",
    "    jets_pt = jets.pt\n",
    "    jets_eta = jets.eta\n",
    "    jets_sin_phi = np.sin(jets.phi)\n",
    "    jets_cos_phi = np.cos(jets.phi)\n",
    "\n",
    "    #Met\n",
    "    mets_sumEt = mets.sumEt #awkward array (no padded needed)\n",
    "    mets_sin_phi = np.sin(mets.phi)\n",
    "    mets_cos_phi = np.cos(mets.phi)\n",
    "    \n",
    "    num_events = ak.num(jets, axis=0)\n",
    "\n",
    "    #initializing (for memory allocation)\n",
    "    leptons_btag = np.zeros(num_events)\n",
    "    leptons_mass = np.zeros(num_events)\n",
    "    leptons_pt = np.zeros(num_events)\n",
    "    leptons_eta = np.zeros(num_events)\n",
    "    leptons_sin_phi = np.zeros(num_events)\n",
    "    leptons_cos_phi = np.zeros(num_events)\n",
    "\n",
    "    # creating lepton mask (either one electron or one muon)\n",
    "    has_electron = ak.num(electrons, axis=1) > 0\n",
    "    has_muon = ak.num(muons, axis=1) > 0\n",
    "\n",
    "    #filling electrons features from events\n",
    "    leptons_mass[has_electron] = electrons.mass[has_electron][:, 0]\n",
    "    leptons_pt[has_electron] = electrons.pt[has_electron][:, 0]\n",
    "    leptons_eta[has_electron] = electrons.eta[has_electron][:, 0]\n",
    "    leptons_sin_phi[has_electron] = np.sin(electrons.phi[has_electron][:, 0])\n",
    "    leptons_cos_phi[has_electron] = np.cos(electrons.phi[has_electron][:, 0])\n",
    "    #fillinig  muons features from events\n",
    "    leptons_mass[has_muon] = muons.mass[has_muon][:, 0]\n",
    "    leptons_pt[has_muon] = muons.pt[has_muon][:, 0]\n",
    "    leptons_eta[has_muon] = muons.eta[has_muon][:, 0]\n",
    "    leptons_sin_phi[has_muon] = np.sin(muons.phi[has_muon][:, 0])\n",
    "    leptons_cos_phi[has_muon] = np.cos(muons.phi[has_muon][:, 0])\n",
    "    \n",
    "\n",
    "    # Combining momenta (leptons + jets)\n",
    "    max_num_jets = ak.max(ak.num(jets, axis=1))\n",
    "    if max_num_jets > 16:\n",
    "        print('Maximum num_jets exceed 16: ', max_num_jets, output_file)\n",
    "        continue\n",
    "#         break\n",
    "        \n",
    "    max_num_jets = 16\n",
    "    \n",
    "    momenta_btag = np.zeros((num_events, max_num_jets + 1)) #initializing 2d array. num_events*(max_num_jets+1 lepton) (padded)\n",
    "    momenta_mass = np.zeros((num_events, max_num_jets + 1))\n",
    "    momenta_pt = np.zeros((num_events, max_num_jets + 1))\n",
    "    momenta_eta = np.zeros((num_events, max_num_jets + 1))\n",
    "    momenta_sin_phi = np.zeros((num_events, max_num_jets + 1))\n",
    "    momenta_cos_phi = np.zeros((num_events, max_num_jets + 1))\n",
    "\n",
    "    #the 0th argument from lepton information\n",
    "    momenta_btag[:, 0] = leptons_btag #no btag for electron. Thus, all zero.\n",
    "    momenta_mass[:, 0] = leptons_mass\n",
    "    momenta_pt[:, 0] = leptons_pt\n",
    "    momenta_eta[:, 0] = leptons_eta\n",
    "    momenta_sin_phi[:, 0] = leptons_sin_phi\n",
    "    momenta_cos_phi[:, 0] = leptons_cos_phi\n",
    "\n",
    "    #filling jets into momenta\n",
    "    for idx in range(max_num_jets): #going through jets level\n",
    "        mask = ak.num(jets, axis=1) > idx #filtering events that has num_jets > idx\n",
    "        momenta_btag[mask, idx + 1] = jets_btag[mask][:, idx]\n",
    "        momenta_mass[mask, idx + 1] = jets_mass[mask][:, idx]\n",
    "        momenta_pt[mask, idx + 1] = jets_pt[mask][:, idx]\n",
    "        momenta_eta[mask, idx + 1] = jets_eta[mask][:, idx]\n",
    "        momenta_sin_phi[mask, idx + 1] = jets_sin_phi[mask][:, idx]\n",
    "        momenta_cos_phi[mask, idx + 1] = jets_cos_phi[mask][:, idx]\n",
    "\n",
    "    momenta_MASK = momenta_mass != 0 #this should valid (since mass would never be zero -> jets/leptons exist)\n",
    "\n",
    "    #convert met to numpy\n",
    "    mets_sumEt = mets_sumEt.to_numpy()\n",
    "    mets_sin_phi = mets_sin_phi.to_numpy()\n",
    "    mets_cos_phi = mets_cos_phi.to_numpy()\n",
    "    \n",
    "    ### TRAGET indices ###\n",
    "    #ht\n",
    "    b_had_indices = np.zeros(num_events, dtype=int)\n",
    "    q1_indices = np.zeros(num_events, dtype=int)\n",
    "    q2_indices = np.zeros(num_events, dtype=int)\n",
    "    #lt\n",
    "    b_lep_indices = np.zeros(num_events, dtype=int)\n",
    "    l_indices = np.zeros(num_events, dtype=int)\n",
    "\n",
    "    b_lep_indices[:] = ak.argmax(labels == -6, axis=1) + 1 #avoiding for-loop since there is exact one -6 and one 6\n",
    "    b_had_indices[:] = ak.argmax(labels == 6, axis=1) + 1\n",
    "\n",
    "    # Find indices for 24\n",
    "    for idx, event in enumerate(labels):\n",
    "        q_indices = ak.where(event == 24)[0]\n",
    "        q1_indices[idx] = q_indices[0] + 1\n",
    "        q2_indices[idx] = q_indices[1] + 1\n",
    "    \n",
    "        \n",
    "    ### Preparing data for h5 file ###\n",
    "    #INPUTS\n",
    "    ##Momenta\n",
    "    momenta_data = {\n",
    "        'MASK': momenta_MASK, # capital MASK\n",
    "        'mass': momenta_mass,\n",
    "        'pt': momenta_pt,\n",
    "        'eta': momenta_eta,\n",
    "        'cos_phi': momenta_cos_phi,\n",
    "        'sin_phi': momenta_sin_phi,\n",
    "        'btag': momenta_btag,\n",
    "    }\n",
    "    ##Met\n",
    "    met_data = {\n",
    "        'sumet': mets_sumEt,\n",
    "        'cos_phi': mets_cos_phi,\n",
    "        'sin_phi': mets_sin_phi,\n",
    "    }\n",
    "\n",
    "\n",
    "    #TARGETS\n",
    "    ht_target = {\n",
    "        'b': b_had_indices,\n",
    "        'q1': q1_indices,\n",
    "        'q2': q2_indices,\n",
    "    }\n",
    "\n",
    "    lt_target = {\n",
    "        'b': b_lep_indices,\n",
    "        'l': l_indices,\n",
    "    }\n",
    "    \n",
    "#creating h5 file\n",
    "    \n",
    "    # Define the directory and file name\n",
    "    directory = \"events_h5_files/scaleup_h5_16maxJets\"\n",
    "\n",
    "#     file_name = \"your_file_name.h5\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "#     os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Full path to the HDF5 file\n",
    "    file_path = os.path.join(directory, output_file)\n",
    "\n",
    "#     print(f\"HDF5 file written to {file_path}\")\n",
    "\n",
    "    \n",
    "    with h5py.File(file_path, 'w') as h5file:\n",
    "        #INPUTS\n",
    "        input_group = h5file.create_group('INPUTS')\n",
    "        momenta_subgroup = input_group.create_group('Momenta')\n",
    "        met_subgroup = input_group.create_group('Met')\n",
    "\n",
    "        for key, value in momenta_data.items():\n",
    "            momenta_subgroup.create_dataset(key, data=value)\n",
    "        for key, value in met_data.items():\n",
    "            met_subgroup.create_dataset(key, data=value)\n",
    "\n",
    "        #EVENT\n",
    "        event_group = h5file.create_group('TARGETS')\n",
    "        ht_subgroup = event_group.create_group('ht')\n",
    "        lt_subgroup = event_group.create_group('lt')\n",
    "\n",
    "        for key, value in ht_target.items():\n",
    "            ht_subgroup.create_dataset(key, data=value)\n",
    "\n",
    "        for key, value in lt_target.items():\n",
    "            lt_subgroup.create_dataset(key, data=value)\n",
    "\n",
    "        #PERMUTATION\n",
    "        perm_group = h5file.create_group('PERMUTATION')\n",
    "\n",
    "        #REGRESSION\n",
    "        regression_group = h5file.create_group('REGRESSION')\n",
    "\n",
    "        #CLASSIFICATION\n",
    "        classification_group = h5file.create_group('CLASSIFICATION')\n",
    "    \n",
    "    print(output_file,': convertion done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum num_jets exceed 16:  40 training_SPANet_structure_ext4-v1_00000_0024.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPANet Demo Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for training_SPANet_structure_ext4-v1_00000_0008.h5 \n",
      "============================================================\n",
      "\n",
      "|-CLASSIFICATION                \n",
      "|-INPUTS                        \n",
      "|---Met                         \n",
      "|-----cos_phi                    :: float32  : (8923,)\n",
      "|-----sin_phi                    :: float32  : (8923,)\n",
      "|-----sumet                      :: float32  : (8923,)\n",
      "|---Momenta                     \n",
      "|-----MASK                       :: bool     : (8923, 17)\n",
      "|-----btag                       :: float64  : (8923, 17)\n",
      "|-----cos_phi                    :: float64  : (8923, 17)\n",
      "|-----eta                        :: float64  : (8923, 17)\n",
      "|-----mass                       :: float64  : (8923, 17)\n",
      "|-----pt                         :: float64  : (8923, 17)\n",
      "|-----sin_phi                    :: float64  : (8923, 17)\n",
      "|-PERMUTATION                   \n",
      "|-REGRESSION                    \n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (8923,)\n",
      "|-----q1                         :: int64    : (8923,)\n",
      "|-----q2                         :: int64    : (8923,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (8923,)\n",
      "|-----l                          :: int64    : (8923,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py training_SPANet_structure_ext4-v1_00000_0008.h5 --shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for /eos/user/n/nmuangko/semi_leptonnic_ttbar/data_h5_13maxJets/training_SPANet_structure_ext4-v1_00000_0007.h5 \n",
      "============================================================\n",
      "\n",
      "|-CLASSIFICATION                \n",
      "|-INPUTS                        \n",
      "|---Met                         \n",
      "|-----cos_phi                    :: float32  : (9832,)\n",
      "|-----sin_phi                    :: float32  : (9832,)\n",
      "|-----sumet                      :: float32  : (9832,)\n",
      "|---Momenta                     \n",
      "|-----MASK                       :: bool     : (9832, 14)\n",
      "|-----btag                       :: float64  : (9832, 14)\n",
      "|-----cos_phi                    :: float64  : (9832, 14)\n",
      "|-----eta                        :: float64  : (9832, 14)\n",
      "|-----mass                       :: float64  : (9832, 14)\n",
      "|-----pt                         :: float64  : (9832, 14)\n",
      "|-----sin_phi                    :: float64  : (9832, 14)\n",
      "|-PERMUTATION                   \n",
      "|-REGRESSION                    \n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (9832,)\n",
      "|-----q1                         :: int64    : (9832,)\n",
      "|-----q2                         :: int64    : (9832,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (9832,)\n",
      "|-----l                          :: int64    : (9832,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/data_h5_13maxJets/training_SPANet_structure_ext4-v1_00000_0007.h5 --shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SPANet Data/Model Construction (based on the demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud h5 files reading test\n",
    "train_filepath_cloud = '/eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation_subset.h5'\n",
    "test_filepath_cloud = '/eos/user/n/nmuangko/semi_leptonnic_ttbar/testing_mass_variation_subset.h5'\n",
    "# filepath_test = 'testing_mass_variation.h5' #path from SWAN\n",
    "# filepath_train = 'training_mass_variation.h5'\n",
    "train_file = h5py.File(train_filepath_cloud,'r')\n",
    "test_file = h5py.File(test_filepath_cloud,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KeysViewHDF5 ['INPUTS', 'REGRESSIONS', 'TARGETS']>,\n",
       " <KeysViewHDF5 ['INPUTS', 'REGRESSIONS', 'TARGETS']>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.keys(), test_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file['INPUTS']['Momenta']['MASK'][:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_side = train_file['TARGETS']['ht']\n",
    "lt_side = train_file['TARGETS']['lt']\n",
    "\n",
    "b_ht = ht_side['b']\n",
    "q1_ht = ht_side['q1']\n",
    "q2_ht = ht_side['q2']\n",
    "b_lt = lt_side['b']\n",
    "l_lt = lt_side['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadronic side::     b: 3  q1: 1  q2: 6\n",
      "Leptonic side::     b: 2  l: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "print('Hadronic side::   ', ' b:', b_ht[idx], ' q1:', q1_ht[idx], ' q2:', q2_ht[idx])\n",
    "print('Leptonic side::   ', ' b:', b_lt[idx], ' l:', l_lt[idx])\n",
    "# indices of lepton is always 0 because all event has one electron (need to indicate in .ymal so that lepton_target is associated with lepton_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, -1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file['TARGETS']['ht']['b'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "| Structure for /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 \n",
      "============================================================\n",
      "\n",
      "|-INPUTS                        \n",
      "|---Met                         \n",
      "|-----cos_phi                    :: float32  : (12427644,)\n",
      "|-----met                        :: float32  : (12427644,)\n",
      "|-----sin_phi                    :: float32  : (12427644,)\n",
      "|-----sumet                      :: float32  : (12427644,)\n",
      "|---Momenta                     \n",
      "|-----MASK                       :: bool     : (12427644, 16)\n",
      "|-----btag                       :: float32  : (12427644, 16)\n",
      "|-----cos_phi                    :: float32  : (12427644, 16)\n",
      "|-----eta                        :: float32  : (12427644, 16)\n",
      "|-----etag                       :: float32  : (12427644, 16)\n",
      "|-----mass                       :: float32  : (12427644, 16)\n",
      "|-----pt                         :: float32  : (12427644, 16)\n",
      "|-----qtag                       :: float32  : (12427644, 16)\n",
      "|-----rapidity                   :: float32  : (12427644, 16)\n",
      "|-----sin_phi                    :: float32  : (12427644, 16)\n",
      "|-----utag                       :: float32  : (12427644, 16)\n",
      "|-REGRESSIONS                   \n",
      "|---EVENT                       \n",
      "|-----invariant_mass             :: float32  : (12427644,)\n",
      "|-----log_invariant_mass         :: float32  : (12427644,)\n",
      "|-----log_neutrino_eta           :: float32  : (12427644,)\n",
      "|-----log_neutrino_px            :: float32  : (12427644,)\n",
      "|-----log_neutrino_py            :: float32  : (12427644,)\n",
      "|-----log_neutrino_pz            :: float32  : (12427644,)\n",
      "|-----mt                         :: float32  : (12427644,)\n",
      "|-----mx                         :: float32  : (12427644,)\n",
      "|-----neutrino_eta               :: float32  : (12427644,)\n",
      "|-----neutrino_px                :: float32  : (12427644,)\n",
      "|-----neutrino_py                :: float32  : (12427644,)\n",
      "|-----neutrino_pz                :: float32  : (12427644,)\n",
      "|---ht                          \n",
      "|-----PARTICLE                  \n",
      "|-------invariant_mass           :: float32  : (12427644,)\n",
      "|-------log_invariant_mass       :: float32  : (12427644,)\n",
      "|---lt                          \n",
      "|-----PARTICLE                  \n",
      "|-------invariant_mass           :: float32  : (12427644,)\n",
      "|-------log_invariant_mass       :: float32  : (12427644,)\n",
      "|-TARGETS                       \n",
      "|---ht                          \n",
      "|-----b                          :: int64    : (12427644,)\n",
      "|-----q1                         :: int64    : (12427644,)\n",
      "|-----q2                         :: int64    : (12427644,)\n",
      "|---lt                          \n",
      "|-----b                          :: int64    : (12427644,)\n",
      "|-----l                          :: int64    : (12427644,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 --shape\n",
    "# python utils/examine_hdf5.py /eos/user/n/nmuangko/semi_leptonnic_ttbar/training_mass_variation.h5 --shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt_dict, ht_dict = check_target_label(f_test)\n",
    "\n",
    "# for key, value in lt_dict.items():\n",
    "#     print('Leptonic top side:', key, value)\n",
    "# for key, value in ht_dict.items():\n",
    "#     print('Hadronic top side:', key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### reducing testing and training size to avoid kernal crash\n",
    "# def copy_group(source_group, target_group, n_events_to_copy):\n",
    "#     for key in source_group.keys():\n",
    "#         item = source_group[key]\n",
    "#         if isinstance(item, h5py.Group):\n",
    "#             # Create the group in the target file\n",
    "#             print(f\"Creating group {key}\")\n",
    "#             new_group = target_group.create_group(key)\n",
    "#             # Recursively copy the subgroup\n",
    "#             copy_group(item, new_group, n_events_to_copy)\n",
    "#         elif isinstance(item, h5py.Dataset):\n",
    "#             print(f\"Copying dataset {key}\")\n",
    "#             # Copy the dataset with a subset of data\n",
    "#             if item.ndim == 1:\n",
    "#                 target_group.create_dataset(key, data=item[:n_events_to_copy])\n",
    "#             else:\n",
    "#                 target_group.create_dataset(key, data=item[:n_events_to_copy, :])\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown item type for {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newfile_path_test_subset = 'testing_mass_variation_subset_v2.h5'\n",
    "# newfile_path_train_subset = 'training_mass_variation_subset_v2.h5'\n",
    "\n",
    "# ratio = 0.8\n",
    "# test_num_events = int(ratio*(1865837))\n",
    "# train_num_events = int(ratio*(12427644))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the original HDF5 file\n",
    "# with h5py.File(train_filepath_cloud, 'r') as train_file:\n",
    "#     # Create a new HDF5 file\n",
    "#     with h5py.File(newfile_path_train_subset, 'w') as new_train_file:\n",
    "#         # Copy the structure and a subset of data\n",
    "#         copy_group(train_file, new_train_file, train_num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(test_filepath_cloud, 'r') as test_file:\n",
    "#     # Create a new HDF5 file\n",
    "#     with h5py.File(newfile_path_test_subset, 'w') as new_test_file:\n",
    "#         # Copy the structure and a subset of data\n",
    "#         copy_group(test_file, new_test_file, test_num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python utils/examine_hdf5.py training_mass_variation_subset_v2.h5 --shape"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
